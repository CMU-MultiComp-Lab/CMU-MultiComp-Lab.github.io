---
layout: fall2023/page
permalink: /mmml-course/fall2023/readings/
title: Readings
---

**Week 2**

* Paper A: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 3_
* Paper B: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 4_
* Paper C: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 5_
* Paper D: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 6_
* Paper E: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 7_
* Paper F: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 8_


**Week 3**

* Paper A: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)
* Paper B: [Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!](https://aclanthology.org/2020.emnlp-main.62.pdf)
* Paper C: [Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions](https://aclanthology.org/2022.findings-emnlp.344/)
* Paper D: [What Makes Training Multi-modal Classification Networks Hard?](https://arxiv.org/pdf/1905.12681.pdf)


**Week 5**

* Paper A: [Improving Multimodal fusion via Mutual Dependency Maximisation](https://aclanthology.org/2021.emnlp-main.21.pdf)
* Paper B: [Deep Multimodal Clustering for Unsupervised Audiovisual Learning](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.pdf)
* Paper C: [VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment](https://arxiv.org/pdf/2210.04135.pdf)
* Paper D: [Are Multimodal Transformers Robust to Missing Modality?](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)


**Week 7**

* Paper A: [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf)
* Paper B: [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)[](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)
* Paper C: [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/pdf/2301.13823.pdf)
* Paper D: [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://arxiv.org/pdf/2010.06775.pdf)
* Paper 1 (optional): [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/pdf/2309.05519.pdf)
* Paper 2 (optional): [Grounding ‘Grounding’ in NLP](https://arxiv.org/abs/2106.02192)
* Paper 3 (optional): [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)
* Paper 4 (optional): [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/pdf/2307.15818.pdf)
* Paper 5 (optional): [Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control](https://arxiv.org/pdf/2303.00855.pdf)
