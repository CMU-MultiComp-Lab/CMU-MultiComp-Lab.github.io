---
layout: fall2023/page
permalink: /fall2023/readings/
title: Readings
---

**Week 2**

* Paper A: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 3_
* Paper B: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 4_
* Paper C: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 5_
* Paper D: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 6_
* Paper E: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 7_
* Paper F: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 8_


**Week 3**

* Paper A: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)
* Paper B: [Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!](https://aclanthology.org/2020.emnlp-main.62.pdf)
* Paper C: [Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions](https://aclanthology.org/2022.findings-emnlp.344/)
* Paper D: [What Makes Training Multi-modal Classification Networks Hard?](https://arxiv.org/pdf/1905.12681.pdf)


**Week 5**

* Paper A: [Improving Multimodal fusion via Mutual Dependency Maximisation](https://aclanthology.org/2021.emnlp-main.21.pdf)
* Paper B: [Deep Multimodal Clustering for Unsupervised Audiovisual Learning](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.pdf)
* Paper C: [VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment](https://arxiv.org/pdf/2210.04135.pdf)
* Paper D: [Are Multimodal Transformers Robust to Missing Modality?](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)


**Week 7**

* Paper A: [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf)
* Paper B: [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)[](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)
* Paper C: [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/pdf/2301.13823.pdf)
* Paper D: [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://arxiv.org/pdf/2010.06775.pdf)
* Paper 1 (optional): [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/pdf/2309.05519.pdf)
* Paper 2 (optional): [Grounding ‘Grounding’ in NLP](https://arxiv.org/abs/2106.02192)
* Paper 3 (optional): [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)
* Paper 4 (optional): [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/pdf/2307.15818.pdf)
* Paper 5 (optional): [Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control](https://arxiv.org/pdf/2303.00855.pdf)


**Week 11**

* Paper A: [VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning](https://arxiv.org/abs/2102.10407)
* Paper B: [MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA](https://arxiv.org/pdf/2209.14792.pdf)
* Paper C: [Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
* Paper D: [Identifying Implicit Social Biases in Vision-Language Models](https://openreview.net/forum?id=LOkEuKq7K1) [Note: Major part starts after the references, page 9]
* Paper 1 (optional): [Text-to-image Diffusion Models in Generative AI: A Survey](https://arxiv.org/abs/2303.07909)
* Paper 2 (optional): [RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model](https://arxiv.org/abs/2309.00810)
* Paper 3 (optional): [MusicLM: Generating Music From Text](https://arxiv.org/abs/2301.11325)
* Paper 4 (optional): [MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models](https://arxiv.org/pdf/2303.12734.pdf)
* Paper 5 (optional): [Making the Most of Biased Samples via Contrastive Learning](https://aclanthology.org/2022.findings-emnlp.495.pdf)

**Week 12**

* Paper A: [Characterizing and Overcoming the Greedy Nature of Learning in Multi-modal Deep Neural Networks](https://proceedings.mlr.press/v162/wu22d.html)
* Paper B: [Provable Dynamic Fusion for Low-Quality Multimodal Data](https://arxiv.org/pdf/2306.02050.pdf)
* Paper C: [What Makes Multi-modal Learning Better than Single (Provably)](https://arxiv.org/abs/2106.04538)
* Paper D: [Factorized Contrastive Learning: Going Beyond Multi-view Redundancy](https://arxiv.org/abs/2306.05268)
* Paper 1 (optional): [Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably)](https://proceedings.mlr.press/v162/huang22e.html)
* Paper 2 (optional): [Ten myths of multimodal interaction](https://dl.acm.org/doi/abs/10.1145/319382.319398)
