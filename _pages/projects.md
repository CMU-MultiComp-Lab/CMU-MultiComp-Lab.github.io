---
layout: page
permalink: /fall2020/projects/
title: Course Project
description: Guidelines and suggestions for course projects
---

**Previous course projects:**

1. [Papers in NIPS 2017 workshop on disentangled representation learning](https://sites.google.com/view/disentanglenips2017){:target="\_blank"}.
2. Denton et al., [Unsupervised Learning of Disentangled Representations from Video](https://papers.nips.cc/paper/7028-unsupervised-learning-of-disentangled-representations-from-video){:target="\_blank"}. NIPS 2017.
3. Chen et al., [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657){:target="\_blank"}. NIPS 2016.
4. Kulkarni et al., [Deep Convolutional Inverse Graphics Network](https://arxiv.org/abs/1503.03167){:target="\_blank"}. NIPS 2015.

1. Chaplot et al., [Gated-Attention Architectures for Task-Oriented Language Grounding](https://arxiv.org/abs/1706.07230). AAAI 2018
2. Liu et al., [Efficient Low-rank Multimodal Fusion with Modality-Specific Factors](https://arxiv.org/abs/1806.00064). ACL 2018
3. Ravichander et al., [Preserving Intermediate Objectives: One Simple Trick to Improve Learning for Hierarchical Models](https://arxiv.org/abs/1706.07867). 
4. Wang et al., [Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/1609.05244).
5. Patel et al., [Self-Supervised Visual Representations for Cross-Modal Retrieval](https://arxiv.org/abs/1902.00378)
6. Pham et al., [Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities](https://arxiv.org/abs/1812.07809). AAAI 2019
7. Chen et al., [Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning](https://arxiv.org/abs/1802.00924). ICMI 2017
8. Sharma et al., [Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models](https://nips2018vigil.github.io/static/papers/accepted/33.pdf). NeurIPS workshop 2018

***
