---
layout: fall2022/page
permalink: /mmml-course/fall2022/readings/
title: Readings
---

Week 2:

1. **Paper A**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 3_
    
2. **Paper B**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 4_
    
3. **Paper C**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 5_
    
4. **Paper D**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 6_
    
5. **Paper E**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 7_
6. **Paper F**: [Foundations & Recent Trends in Multimodal Machine Learning Definitions, Challenges, & Open Questions](https://arxiv.org/pdf/2209.03430.pdf) - _Section 1, Section 2, Section 8_

Week 3:
1. Zeiler and Fergus, [Visualizing and Understanding Convolutional Networks](https://piazza.com/class_profile/get_resource/jjyt9xcoem64k5/jlvnkpiszoo26g). ECCV 2014
2. Selvaraju et al., [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://piazza.com/class_profile/get_resource/jjyt9xcoem64k5/jlscu1vibjh3s8). ICCV 2017
3. Karpathy et al., [Visualizing and Understanding Recurrent Networks](https://arxiv.org/pdf/1506.02078.pdf). arXiv 2015
4. Khandelwal et al., [Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context](https://arxiv.org/pdf/1805.04623.pdf). ACL 2018
5.  **(optional reading)** [Learning Translation Invariance in CNNs](https://arxiv.org/pdf/2011.11757.pdf)

Week 5:
**For this week, you are expected to read one of the following papers:**

1.   **Paper A:** [Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection](https://openaccess.thecvf.com/content/WACV2022/papers/Pramanick_Multimodal_Learning_Using_Optimal_Transport_for_Sarcasm_and_Humor_Detection_WACV_2022_paper.pdf)
2.   **Paper B:** [Deep Multimodal Clustering for Unsupervised Audiovisual Learning](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.pdf)
3.   **Paper C:** [On the Benefits of Early Fusion in Multimodal Representation Learning](https://arxiv.org/pdf/2011.07191.pdf)
4.   **Paper D:** [Improving Multimodal fusion via Mutual Dependency Maximisation](https://arxiv.org/pdf/2109.00922.pdf) 

(Optional Readings)

1.   [Multiplicative Interactions and Where to Find Them](https://openreview.net/pdf?id=rylnK6VtDH)
2.   [Self-Supervised Learning from a Multi-View Perspective](https://arxiv.org/pdf/2006.05576.pdf)
3.   [Learning Factorized Multimodal Representations](https://arxiv.org/pdf/1806.06176.pdf)

