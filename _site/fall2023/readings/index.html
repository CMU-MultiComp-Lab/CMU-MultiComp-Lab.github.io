<div class="post">

  <header class="post-header">
    <h1 class="post-title">Readings</h1>
    <h2 class="post-description"></h2>
  </header>

  <article class="post-content Readings clearfix">
    <p><strong>Week 2</strong></p>

<ul>
  <li>Paper A: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 3</em></li>
  <li>Paper B: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 4</em></li>
  <li>Paper C: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 5</em></li>
  <li>Paper D: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 6</em></li>
  <li>Paper E: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 7</em></li>
  <li>Paper F: <a href="https://arxiv.org/pdf/2209.03430.pdf">Foundations &amp; Recent Trends in Multimodal Machine Learning Definitions, Challenges, &amp; Open Questions</a> - <em>Section 1, Section 2, Section 8</em></li>
</ul>

<p><strong>Week 3</strong></p>

<ul>
  <li>Paper A: <a href="https://arxiv.org/abs/1610.02391">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a></li>
  <li>Paper B: <a href="https://aclanthology.org/2020.emnlp-main.62.pdf">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a></li>
  <li>Paper C: <a href="https://aclanthology.org/2022.findings-emnlp.344/">Beyond Additive Fusion: Learning Non-Additive Multimodal Interactions</a></li>
  <li>Paper D: <a href="https://arxiv.org/pdf/1905.12681.pdf">What Makes Training Multi-modal Classification Networks Hard?</a></li>
</ul>

<p><strong>Week 5</strong></p>

<ul>
  <li>Paper A: <a href="https://aclanthology.org/2021.emnlp-main.21.pdf">Improving Multimodal fusion via Mutual Dependency Maximisation</a></li>
  <li>Paper B: <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.pdf">Deep Multimodal Clustering for Unsupervised Audiovisual Learning</a></li>
  <li>Paper C: <a href="https://arxiv.org/pdf/2210.04135.pdf">VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</a></li>
  <li>Paper D: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf">Are Multimodal Transformers Robust to Missing Modality?</a></li>
</ul>

<p><strong>Week 7</strong></p>

<ul>
  <li>Paper A: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</a></li>
  <li>Paper B: <a href="https://arxiv.org/abs/2204.00598">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</a><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf"></a></li>
  <li>Paper C: <a href="https://arxiv.org/pdf/2301.13823.pdf">Grounding Language Models to Images for Multimodal Inputs and Outputs</a></li>
  <li>Paper D: <a href="https://arxiv.org/pdf/2010.06775.pdf">Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision</a></li>
  <li>Paper 1 (optional): <a href="https://arxiv.org/pdf/2309.05519.pdf">NExT-GPT: Any-to-Any Multimodal LLM</a></li>
  <li>Paper 2 (optional): <a href="https://arxiv.org/abs/2106.02192">Grounding ‘Grounding’ in NLP</a></li>
  <li>Paper 3 (optional): <a href="https://arxiv.org/abs/2302.00923">Multimodal Chain-of-Thought Reasoning in Language Models</a></li>
  <li>Paper 4 (optional): <a href="https://arxiv.org/pdf/2307.15818.pdf">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></li>
  <li>Paper 5 (optional): <a href="https://arxiv.org/pdf/2303.00855.pdf">Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control</a></li>
</ul>

  </article>

</div>
